import os
from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_groq import ChatGroq
from services.chroma_service import ChromaService
from pydantic import BaseModel
from .prompt import system_prompt

from typing import List

load_dotenv()

os.environ["GROQ_API_KEY"] = os.getenv("GROQ_API_KEY")

class MessageState(BaseModel):
    """
    Represents the state of our graph.
    Attributes:
        input: user input
        answer: answer generated by the rag
        documents: list of documents
    """
    input: str
    answer: str | None = None
    documents: List[Document] | None = None

llm = ChatGroq(model="llama3-8b-8192", temperature=0)

rag_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}"),
    ]
)

retriever = ChromaService().load_retriever()
chain = rag_prompt | llm | StrOutputParser()

def retrieve(state: MessageState):
    """
    Retrieve documents
    Args:
        state (dict): The current graph state
    Returns:
        state (dict): New key added to state, documents, that contains retrieved documents
    """
    question = state.input

    documents = retriever.invoke(question)
    return {"documents": documents, "input": question}

def generate(state: MessageState):
    """
    generate response based on docs and input state
    Args:
        state (dict): The current graph state
    Returns:
        state (dict): New key added to state, answer, that contains the generated response
    """
    docs = state.documents
    question = state.input
    response = chain.invoke({"context": docs, "input": question})
    return {"documents": docs, "input": question, "answer": response}
